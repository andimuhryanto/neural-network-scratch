{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1. / (1. + np.exp(-Z))\n",
    "\n",
    "def tanh(Z):\n",
    "    return np.tanh(Z)\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def tanh_derivative(Z):\n",
    "    a = tanh(Z)\n",
    "    return 1 - np.power(a, 2)\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    a = sigmoid(Z)\n",
    "    return a * (1 - a)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return Z > 0 * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_random_weights(m, n):\n",
    "    return np.random.randn(m, n) / np.sqrt(n) #* .01\n",
    "\n",
    "def init_zeros(m):\n",
    "    return np.zeros(shape=(m, 1))\n",
    "\n",
    "def init_params(layer_units):\n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    for i, l in enumerate(range(1, len(layer_units))):\n",
    "        m = layer_units[l]\n",
    "        n = layer_units[l-1]\n",
    "\n",
    "        weights.append(init_random_weights(m, n))\n",
    "        biases.append(init_zeros(m))\n",
    "        \n",
    "        assert(weights[i].shape == (m, n))\n",
    "        assert(biases[i].shape == (m, 1))\n",
    "    \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    \n",
    "    return Z\n",
    "\n",
    "def activation_function(Z, activation):\n",
    "    if activation == \"sigmoid\": return sigmoid(Z)\n",
    "    elif activation == \"relu\": return relu(Z)\n",
    "    elif activation == \"tanh\": return tanh(Z)\n",
    "    else: return Z # Default to identity activation\n",
    "    \n",
    "def forward_propagation(A_prev, W, b, activation):\n",
    "    Z = linear_forward(A=A_prev, W=W, b=b)\n",
    "    A = activation_function(Z=Z, activation=activation)\n",
    "    return A, Z\n",
    "\n",
    "def model_forward_propagation(X, weights, biases, activation_functions, verbose = False):\n",
    "    A = X              # X.T becomes A[0]\n",
    "    L = len(weights)\n",
    "    caches = {'A_prev': []\n",
    "        , 'W': []\n",
    "        , 'b': []\n",
    "        , 'Z': []\n",
    "    }\n",
    "    print('| Forward Propagation |') if verbose else None\n",
    "    for i in range(L):\n",
    "        \n",
    "        A_prev = A.copy()\n",
    "        W = weights[i]\n",
    "        b = biases[i]\n",
    "        activation = activation_functions['hidden'] if i != L-1 else activation_functions['output']\n",
    "        print(f'Iteration-{i} Layer-{i+1}',  activation) if verbose else None\n",
    "        \n",
    "        A, Z = forward_propagation(A_prev, W, b, activation)\n",
    "        \n",
    "        caches['A_prev'].append(A_prev)\n",
    "        caches['W'].append(W)\n",
    "        caches['b'].append(b)\n",
    "        caches['Z'].append(Z)\n",
    "\n",
    "    return A, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost Function\n",
    "\n",
    "def compute_cost(y, y_pred):\n",
    "    m = len(y)\n",
    "    y_pred = y_pred[0]\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    \n",
    "    cost = (-1./m) * np.sum(np.multiply(y, np.log(y_pred)) + np.multiply((1 - y), np.log(1 - y_pred)))\n",
    "    #cost = np.squeeze(cost)\n",
    "    return cost\n",
    "\n",
    "def compute_cost_regularization(y, y_pred, weights, lambda_reg):\n",
    "    m = len(y)\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    cost = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "    regularization_term = 0\n",
    "    for W in weights:\n",
    "        regularization_term += np.sum(np.square(W))\n",
    "\n",
    "    # Add regularization term to the cost\n",
    "    cost = cost + (lambda_reg / (2 * m)) * regularization_term\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dA, m, g_derivative, A_prev, W, b, lambda_reg=None):\n",
    "    dZ = dA * g_derivative\n",
    "    if lambda_reg is None:\n",
    "        dW = np.dot(dZ, A_prev.T) * 1./m\n",
    "    else:\n",
    "        dW = (np.dot(dZ, A_prev.T) + lambda_reg * W) * 1./m\n",
    "    db = np.sum(dZ, axis = 1, keepdims = True) * 1./m\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def derivative_activation(Z, activation):\n",
    "    if activation == \"sigmoid\": return sigmoid_derivative(Z)\n",
    "    elif activation == \"relu\": return relu_derivative(Z)\n",
    "    elif activation == \"tanh\": return tanh_derivative(Z)\n",
    "    else: return 1 # Default to identity activation \n",
    "\n",
    "def model_backward_propagation(A, y, weights, biases, caches, activation_functions, verbose=False, lambda_reg=None):\n",
    "    gradients_A_prev = []\n",
    "    gradients_W = []\n",
    "    gradients_b = []\n",
    "\n",
    "    L = len(weights)\n",
    "    y = y.reshape(A.shape)\n",
    "    m = len(y)\n",
    "    epsilon = 1e-15\n",
    "    A = np.clip(A, epsilon, 1 - epsilon)\n",
    "\n",
    "    dA = - (np.divide(y, A) - np.divide((1 - y), (1 - A)))\n",
    "    \n",
    "    print('| Backward Propagation |') if verbose else None\n",
    "    for i, j in enumerate(reversed(range(L))):\n",
    "        l = j + 1\n",
    "        activation = activation_functions['output'] if i == 0 else activation_functions['hidden']\n",
    "        Z = caches['Z'][j]\n",
    "        A_prev = caches['A_prev'][j]\n",
    "        W = caches['W'][j]\n",
    "        b = caches['b'][j]\n",
    "        dA_prev = dA\n",
    "        \n",
    "        g_derivative =  derivative_activation(Z, activation)\n",
    "        print(f'Iteration-{i} Index-{j} Layer-{l}', activation, Z.shape, g_derivative.shape) if verbose else None\n",
    "        \n",
    "        dA_prev, dW, db = linear_backward(dA_prev, m, g_derivative, A_prev, W, b, lambda_reg=lambda_reg)\n",
    "        gradients_A_prev.insert(0, dA_prev)\n",
    "        gradients_W.insert(0, dW)\n",
    "        gradients_b.insert(0, db)\n",
    "\n",
    "    return gradients_A_prev, gradients_W, gradients_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(weights, biases, gradients_W, gradients_b, learning_rate):\n",
    "    L = len(weights)\n",
    "    for i in range(L):\n",
    "        weights[i] -=  (learning_rate * gradients_W[i])\n",
    "        biases[i] -= (learning_rate * gradients_b[i])\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_train(X, y, activation_functions, learning_rate, weights, biases, verbose=True, lambda_reg=None):\n",
    "    A, caches = model_forward_propagation(X=X, \n",
    "                                        weights=weights, \n",
    "                                        biases=biases, \n",
    "                                        activation_functions=activation_functions,\n",
    "                                        verbose=verbose)\n",
    "    # Computing Cost\n",
    "    if lambda_reg is None:\n",
    "        cost = compute_cost(y=y, y_pred=A)\n",
    "    else:\n",
    "        cost = compute_cost_regularization(y, A, weights, lambda_reg = 0.01)\n",
    "\n",
    "    gradients_A_prev, gradients_W, gradients_b = model_backward_propagation(A, \n",
    "                                                                            y, \n",
    "                                                                            weights, \n",
    "                                                                            biases, \n",
    "                                                                            caches, \n",
    "                                                                            activation_functions, \n",
    "                                                                            verbose=verbose,\n",
    "                                                                            lambda_reg=lambda_reg)\n",
    "    weights, biases = update_parameters(weights, \n",
    "                                        biases, \n",
    "                                        gradients_W, \n",
    "                                        gradients_b, \n",
    "                                        learning_rate)\n",
    "    \n",
    "    return weights, biases, cost, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(X, y, layer_units, activation_functions, learning_rate, num_iteration, verbose=False, lambda_reg=None):\n",
    "    print('='*75)\n",
    "    print(f\"Train Neural Network Model\")\n",
    "    print('='*75)\n",
    "    print(f\"\\tlayer_units: {layer_units} \")\n",
    "    print(f\"\\tactivation_functions: {activation_functions}\")\n",
    "    print(f\"\\tlearning_rate: {learning_rate}\")\n",
    "    print(f\"\\tnum_iteration: {num_iteration}\")\n",
    "    print('='*75)\n",
    "\n",
    "    weights, biases = init_params(layer_units=layer_units)\n",
    "    costs = []\n",
    "    best_cost = 1.\n",
    "    best_weights = None\n",
    "    best_biases = None\n",
    "    best_iteration = 0\n",
    "\n",
    "    for i in range(num_iteration):\n",
    "        weights, biases, cost, caches = epoch_train(X, y, activation_functions, learning_rate, weights, biases, verbose=verbose, lambda_reg=lambda_reg)\n",
    "        if best_cost > cost:\n",
    "            best_cost = cost\n",
    "            best_iteration, best_weights, best_biases = i+1, weights, biases\n",
    "        if (i+1) % 100 == 0 or i == 0:\n",
    "            print(f'Iteration {i+1}\\tCost: {cost}')\n",
    "        costs.append(cost)\n",
    "    \n",
    "    best_parameters = {\n",
    "        'weights': best_weights,\n",
    "        'biases': best_biases\n",
    "    }\n",
    "    \n",
    "    print('='*75)\n",
    "    print('\\tBest Iteration:', best_iteration, 'Cost Function:', best_cost)\n",
    "    print('='*75)\n",
    "\n",
    "    return best_parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters, activation_functions):\n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m))\n",
    "    best_weights = parameters['weights']\n",
    "    best_biases = parameters['biases']\n",
    "    \n",
    "    probs, caches = model_forward_propagation(X=X, \n",
    "                                              weights=best_weights, \n",
    "                                              biases=best_biases, \n",
    "                                              activation_functions=activation_functions,\n",
    "                                              verbose=False)\n",
    "    for i in range(0, probs.shape[1]):\n",
    "        if probs[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Train Neural Network Model\n",
      "===========================================================================\n",
      "\tlayer_units: [30, 16, 64, 4, 2, 1] \n",
      "\tactivation_functions: {'hidden': 'tanh', 'output': 'sigmoid'}\n",
      "\tlearning_rate: 0.0001\n",
      "\tnum_iteration: 1000\n",
      "===========================================================================\n",
      "Iteration 1\tCost: 0.7385391896445938\n",
      "Iteration 100\tCost: 0.16149060034620003\n",
      "Iteration 200\tCost: 0.10075712696483807\n",
      "Iteration 300\tCost: 0.08852392005030764\n",
      "Iteration 400\tCost: 0.06904333713378183\n",
      "Iteration 500\tCost: 0.05999052547649123\n",
      "Iteration 600\tCost: 0.05401788472122086\n",
      "Iteration 700\tCost: 0.049707189571740654\n",
      "Iteration 800\tCost: 0.04644802932610347\n",
      "Iteration 900\tCost: 0.04393146284303655\n",
      "Iteration 1000\tCost: 0.0419626281235537\n",
      "===========================================================================\n",
      "\tBest Iteration: 1000 Cost Function: 0.0419626281235537\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "## Prepare the Dataset\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True, stratify=y)\n",
    "X_train, X_test = X_train.T, X_test.T\n",
    "\n",
    "# Set Hyperparameters\n",
    "layer_units = [X_train.shape[0], 16, 64, 4, 2, 1] # Number of neuron units for each layers \n",
    "activation_functions = {'hidden': 'tanh', 'output': 'sigmoid'}\n",
    "learning_rate = .0001\n",
    "num_iteration = 1000\n",
    "lambda_reg = 0.01\n",
    "\n",
    "# Run Training\n",
    "best_parameters, costs = train_nn(X_train, y_train, \n",
    "                                  layer_units, \n",
    "                                  activation_functions, \n",
    "                                  learning_rate, \n",
    "                                  num_iteration=num_iteration, \n",
    "                                  lambda_reg=lambda_reg\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix:\n",
      " [[ 62   2]\n",
      " [  5 102]]\n",
      "accuracy_score 0.9590643274853801\n",
      "f1_score 0.966824644549763\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X_test, y_test, parameters=best_parameters, activation_functions=activation_functions)[0]\n",
    "print('confusion_matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "print('accuracy_score', accuracy_score(y_test, y_pred))\n",
    "print('f1_score', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
